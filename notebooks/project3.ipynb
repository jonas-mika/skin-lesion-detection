{
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Project 3: Image Recognition in Medical Treatment\n## Diagnosing Skin Lesions using Machine Learning in Image Processing \n---\n\n**Group 9: Aidan Stocks, Hugo Reinicke, Nicola Clark, Jonas-Mika Senghaas**\n\nSubmission: *19.04.2021* / Last Modified: *08.04.2021*\n\n---\n\nThis notebook contains the step-by-step data science process performed on the *ISIC 2017* public test data and official training data on medical image recognition. The goal of this project was to extract and automatically analyse features from medical images of skin lesions in order to predict whether or not the person has ** using machine learning and image processing.\n\nThe initial data (containing the medical images, masked images and information on features and disease) was given for 150 medical images (equivalent to the public test data of the *ISIC 2017* challenge) by the project manager *Veronika *.\nTo develop more accurate models, we extended the initially given data by the official training data that could be obtained from the official [ISIC 2017 Website](https://challenge.isic-archive.com/data)",
      "metadata": {
        "tags": [],
        "cell_id": "00000-cb34c893-19b5-4eb7-bf5a-d02ead8d44fe",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "## Introduction\n---\nThe amount of medical imaging - just as data in any other field - has increased tremendously within the last decade, making it more and more difficult to manually inspect medical images for diagnosis purposes.\n\nFurthermore, people have proven to be hesitant of visiting doctors because of seemingly 'light' issues, which did not seem to be important enough to occupy a doctor's time. With skin diseases being especially effective in treatment if detected early, this is fatal. \nAn easy-to-use app that implements automated detection of skin diseases from the sofa, would address this issue - ultimately saving lives.",
      "metadata": {
        "tags": [],
        "cell_id": "00001-0c890553-5249-4dc4-b863-ecba0193daab",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "## Running this Notebook\n---\nThis notebook contains all code to reproduce the findings of the project as can be seen on the [GitHub](https://github.com/jonas-mika/fyp2021p03g09) page of this project. In order to read in the data correctly, the global paths configured in the section `Constants` need to be correct. The following file structure - as prepared in the `submission.zip` - was followed throughout the project and is recommended to use (alternatively the paths in the section `Constants` can be adjusted):\n\n```\n*project tree structure*\n```\n*Note that the rest of the file structure as can be seen on the [GitHub](https://github.com/jonas-mika/fyp2021p03g09) page of the project generates automatically*",
      "metadata": {
        "tags": [],
        "cell_id": "00002-9d760373-d759-41b1-9739-48d3452591b9",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "## Required Libraries and Further Imports\n---\nThroughout the project, we will use a range of both built-in and external Python Libraries. This notebook will only run if all libraries and modules are correctly installed on your local machines. \nTo install missing packages use `pip install <package_name>` (PIP (Python Package Index) is the central package management system, read more [here](https://pypi.org/project/pip/)). \n\nIn case you desire further information about the used packages, click the following links to find detailed documentations:\n- [Pandas](https://pandas.pydata.org/)\n- [Numpy](https://numpy.org/)\n- [Matplotlib](https://matplotlib.org/stable/index.html)\n- [PIL](https://pillow.readthedocs.io/en/stable/)\n- [SciKit Learn](https://scikit-learn.org/stable/)\n- [SciKit Image](https://scikit-image.org/)\n- [Scipy](https://www.scipy.org/)",
      "metadata": {
        "tags": [],
        "cell_id": "00003-6e74ab3c-05a2-47b0-95c8-bd85fea6fefc",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00004-54b6965a-fb79-43bd-b513-36fb8a2673bf",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "a281805d",
        "execution_millis": 7601,
        "output_cleared": true,
        "execution_start": 1618981516403,
        "deepnote_cell_type": "code"
      },
      "source": "%%capture\n# uncomment lines with uninstalled packages\n\n!pip install scikit-image\n#!pip install scikit-learn\n#!pip install pillow\n#!pip install itertools",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00000-ce265327-d417-47b0-850b-6a4f0ab3a9cf",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "3285c3db",
        "execution_millis": 2637,
        "output_cleared": true,
        "execution_start": 1618981525912,
        "deepnote_cell_type": "code"
      },
      "source": "# python standard libraries\nimport json                                            # data transfer to json format\nimport os                                              # automates saving of export files (figures, summaries, ...)\nimport random                                          # randomness in coloring of plots\nimport re                                              # used for checking dateformat in data cleaning\nimport math                                            # mathematical computations\nimport requests                                        # requesting from website \nimport zipfile                                         # Used to extract zip files\n\n# external libraries\nimport pandas as pd                                    # provides major datastructure pd.DataFrame() to store datasets\nimport numpy as np                                     # used for numerical calculations and fast array manipulations\nimport matplotlib.pyplot as plt                        # visualisation of data\nimport seaborn as sns\nfrom PIL import Image                                  # fork from PIL (python image library), deals with images in python\n\n# specific functions\nimport matplotlib.cm as cm\nfrom skimage import morphology\nfrom scipy.spatial.distance import cdist\nfrom scipy.stats.stats import mode\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split # split data into train and test\nfrom sklearn.preprocessing import StandardScaler # normalise features\nfrom sklearn.neighbors import KNeighborsClassifier # k-nearest neighbour classifier\nfrom sklearn.svm import SVC # support vector machine classifier\nfrom sklearn.naive_bayes import GaussianNB # naive bayes\nfrom sklearn.feature_selection import mutual_info_classif, SelectKBest # Univariate feature selection with mutual information for feature scoring",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Since this project makes heavy use of functions to achieve maximal efficiency, all functions are stored externally in the package structure `project3'. The following imports are necessary for this notebook to run properly.",
      "metadata": {
        "tags": [],
        "cell_id": "00005-b299d8cd-c193-4cb0-8fbb-48aeeae6c417",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00006-906c3773-a166-4b1d-af19-9f85a5be12a2",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "cafd84ba",
        "execution_millis": 24,
        "output_cleared": true,
        "execution_start": 1618937953609,
        "deepnote_cell_type": "code"
      },
      "source": "#from project3.processing import ...\n#from project3.save import ...\n#from project3.features import ...\n#from fyp2021p03g09_functions import *",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "**Remark**: All function used in this project are well documented in their *docstring*. To display the docstring and get an short summary of the function and the specifications of the input argument (including data tupe and small explanation) as well as their return value, type `?<function_name>` in Juptyer.",
      "metadata": {
        "tags": [],
        "cell_id": "00007-4a1d8fd2-0786-4f56-a42a-23b8c1412efb",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00012-b0a63da5-4fc5-41b4-86d4-5705799185d1",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "27b7344f",
        "execution_millis": 3,
        "output_cleared": true,
        "execution_start": 1618981534195,
        "deepnote_cell_type": "code"
      },
      "source": "# global parameters for running the notebooks\nDOWNLOAD_EXTERNAL_IMAGES = False\nPREPROCESS_IMAGES = False\nCOMPUTE_FEATURES = True",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Downloading Additional Data\n---\n*some description*",
      "metadata": {
        "tags": [],
        "cell_id": "00009-ab8c2759-432e-4819-be9d-80d303620266",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00011-00674360-0ea4-4705-9c64-c6d7cf3f146a",
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Running on Local Machine",
      "metadata": {
        "tags": [],
        "cell_id": "00011-c090fbbd-3bef-4b46-8697-cc2fb76102df",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00010-54eef296-5226-4dc5-a9e8-0fdf18a0a487",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "d0f13bc4",
        "execution_millis": 2,
        "execution_start": 1618938659454,
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "source": "if DOWNLOAD_EXTERNAL_IMAGES == True:\n    # load in diagnosis, images and masks\n    for i, url in enumerate(URL.values()):\n        r = requests.get(url, allow_redirects=True) # request object\n        if i == 1: open(PATH['data']['external'] + 'diagnosis.csv', 'wb').write(r.content)\n        elif i == 2: open(PATH['data']['external'] + 'masks.zip', 'wb').write(r.content)\n        elif i == 3: open(PATH['data']['external'] + 'images.zip', 'wb').write(r.content)\n\n    # unzip images and masks\n    for i, key in enumerate(URL.keys()[1:]):\n        with zipfile.ZipFile(PATH['data']['external'] + key + '.zip','r') as zip_ref:\n            zip_ref.extractall(\"../data/external\")\n\n    # delete zips",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Running from Deepnote ",
      "metadata": {
        "tags": [],
        "cell_id": "00013-697ba0d6-8536-4af4-9c63-41bce1e4f673",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00016-0e5674ff-0c14-4245-8b57-19c01fe3859a",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "a8b116d7",
        "execution_millis": 626,
        "execution_start": 1618961861225,
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "source": "!ls /datasets/googledrive ####:(",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Constants\n---\nTo enhance the readibilty, as well as to decrease the maintenance effort, it is useful for bigger projects to define contants that need to be accessed globally throughout the whole notebook in advance. \nThe following cell contains all of those global constants. By convention, we write them in caps (https://www.python.org/dev/peps/pep-0008/#constants)",
      "metadata": {
        "tags": [],
        "cell_id": "00008-8d4d4766-2724-4033-8584-15fe4722f493",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00009-4e13d720-e857-4d9a-8abb-632aa8f19e3c",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "a1d98c23",
        "execution_millis": 14,
        "output_cleared": true,
        "execution_start": 1618981582246,
        "deepnote_cell_type": "code"
      },
      "source": "URL = {}\nURL['diagnosis'] = 'https://isic-challenge-data.s3.amazonaws.com/2017/ISIC-2017_Test_v2_Part3_GroundTruth.csv'\nURL['masks'] = 'https://isic-challenge-data.s3.amazonaws.com/2017/ISIC-2017_Test_v2_Part1_GroundTruth.zip'\nURL['images'] = 'https://isic-challenge-data.s3.amazonaws.com/2017/ISIC-2017_Test_v2_Data.zip'\n\nPATH = {}\nPATH['data'] = {}\nPATH['data']['raw'] = \"../data/raw/\"\nPATH['data']['processed'] = \"../data/processed/\"\nPATH['data']['external'] = \"../data/external/\"\n\nPATH['images'] = 'images/'\nPATH['masks'] = 'masks/'\nPATH['filtered_images'] = 'filtered_images/'\nPATH['dummy_images']= 'dummy_images/'\n\nPATH['reports'] = \"../reports/\"\n\n# filename lookup dictionary storing the most relevant filenames\nFILENAME = {}\n\n# store filename of datasets involved\nFILENAME['diagnosis'] = 'diagnosis.csv'\nFILENAME['features'] = 'features.csv' \n\n# store filenames of different files in the project for easy iteration\nFILENAME['images'] = sorted([image[:-4] for image in os.listdir(PATH['data']['raw'] + PATH['images']) if not re.match('.*super.*', image)])\nFILENAME['masks'] = sorted([mask[:-4] for mask in os.listdir(PATH['data']['raw'] + PATH['masks'])])\n# add filenames for external\nFILENAME['dummy_images'] = sorted([image[:-4] for image in os.listdir(PATH['data']['external'] + PATH['dummy_images'])])\n\n# defining three dictionaries to store data. each dictionary will reference several pandas dataframes\nDATA = {}\n\nNAMES = {}\nNAMES['datasets'] = ['diagnosis', 'features']\nNAMES['images'] = ['images', 'masks']",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "**TASK 0**\n# Image Preprocessing\n---\nIn this section we preprocess our image to make it nicer to deal with them in the later part of the project.\n\n1. Crop Images and Masks to be bound by lesion\n2. Make Width and Length an even number to be able to crop evenly\n3. Maybe save filtered image with color",
      "metadata": {
        "tags": [],
        "cell_id": "00013-3916c960-be31-476a-80d7-841da336db90",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00014-13f4e761-daae-49a7-acf7-41412d5e2a0f",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "23a735da",
        "execution_millis": 0,
        "output_cleared": true,
        "execution_start": 1618981586076,
        "deepnote_cell_type": "code"
      },
      "source": "# helper function to make width and lengths even\ndef make_even(img):\n    if img.size[0] % 2 != 0: #  making number of cols even\n        img = np.array(img)\n        mid = int(img.shape[1] / 2)\n        img = np.delete(img, mid, axis=1)\n        img = Image.fromarray(img)\n\n    if img.size[1] % 2 != 0: # making number of rows even\n        img = np.array(img)\n        mid = int(img.shape[0] / 2)\n        img = np.delete(img, mid, axis=0)\n        img = Image.fromarray(img)\n    \n    return img",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00014-e8cc52ac-d501-404c-b8ab-f75edf0227d4",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "ad6ddc9e",
        "execution_millis": 0,
        "output_cleared": true,
        "execution_start": 1618981586453,
        "deepnote_cell_type": "code"
      },
      "source": "# preprocessing (170 seconds to run)\nif PREPROCESS_IMAGES == True:\n    try: \n        os.makedirs(PATH['data']['processed'] + PATH['images'])\n        os.makedirs(PATH['data']['processed'] + PATH['masks'])\n        os.makedirs(PATH['data']['processed'] + PATH['filtered_images'])\n    except: print('Directories already exist.')\n\n    for i in range(len(FILENAME['images'])):\n        # get image names\n        img_name = FILENAME['images'][i] + '.jpg'\n        mask_name = FILENAME['masks'][i] + '.png'\n\n        # open temporarily\n        img = Image.open(PATH['data']['raw'] + PATH['images'] + img_name)\n        mask = Image.open(PATH['data']['raw'] + PATH['masks'] + mask_name)\n\n        # crop to only store lesion\n        cropped_img = img.crop(mask.getbbox())\n        cropped_mask = mask.crop(mask.getbbox())\n\n        # make width and length even (two cases)\n        cropped_img = make_even(cropped_img)\n        cropped_mask = make_even(cropped_mask)\n\n        # create filtered with color\n        dummy = Image.new(\"RGB\", cropped_img.size, 0)\n        filtered_img = Image.composite(cropped_img, dummy, cropped_mask)\n        \n        # save to '../data/processed' in correct subfolder\n        cropped_img.save(PATH['data']['processed'] + PATH['images'] + img_name)\n        cropped_mask.save(PATH['data']['processed'] + PATH['masks'] + mask_name)\n        filtered_img.save(PATH['data']['processed'] + PATH['filtered_images'] + img_name)\n        \n\n        \n    print('Preprocessing Done')",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "*TASK 0.5*\n# Data Exploration\n\n---\n",
      "metadata": {
        "tags": [],
        "cell_id": "00010-94b50f7c-6711-4349-bbfc-c11ba8d47de0",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "## Loading in Data\n\n---\n\nThe task involves different sources of data, namely:\n\n> **Images**: 150 Medical Images of Skin Lesions\n\n> **Masks**: 150 Binary Masks corresponding to each Image that masks the region of the Skin Lesion\n\n> **Diagnosis**: Dataset storing whether or not the lesion was either *melanoma* or *seborrheic_keratosis* through binary values\n\n> **Features**: Dataset storing the area and perimeter of the skin lesion for each image\n\nWe conveniently load in the csv datasets into individual `Pandas DataFrames` using the built-in pandas method `pd.read_csv()`. We store those in our `DATA` dictionary in the corresponding keys.\n\nAll images and masks are stored as `Image` objects of the `PIL` (*Python Image Library*) for convenient handling of image processing functionality.",
      "metadata": {
        "tags": [],
        "cell_id": "00011-b337fa92-d698-471c-bc5b-f941c1aef692",
        "deepnote_to_be_reexecuted": true,
        "source_hash": "b623e53d",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00013-2900529b-7a4b-4fce-bc3c-11beff12d811",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "e604c12e",
        "execution_millis": 23,
        "output_cleared": true,
        "execution_start": 1618981589938,
        "deepnote_cell_type": "code"
      },
      "source": "# load in raw datasets \nDATA['diagnosis'] = pd.read_csv(PATH['data']['raw'] + FILENAME['diagnosis'])\nDATA['features'] = pd.read_csv(PATH['data']['raw'] + FILENAME['features']) # not used in this project",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00014-b051376d-2529-459d-a246-d2aa138dab82",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "389aa93f",
        "execution_millis": 442,
        "output_cleared": true,
        "execution_start": 1618981592537,
        "deepnote_cell_type": "code"
      },
      "source": "# load in raw images and masks\nDATA['images'] = [Image.open(PATH['data']['processed'] + PATH['images'] + FILENAME['images'][i] + '.jpg') for i in range(len(FILENAME['images']))]\nDATA['masks'] = [Image.open(PATH['data']['processed'] + PATH['masks'] + FILENAME['masks'][i] + '.png') for i in range(len(FILENAME['masks']))]\n\nDATA['generator'] = ( (Image.open(PATH['data']['processed'] + PATH['images'] + FILENAME['images'][i] + '.jpg'),\n                           Image.open(PATH['data']['processed'] + PATH['masks'] + FILENAME['masks'][i] + '.png')) \n                           for i in range(len(FILENAME['images'])) )\n\n#DATA['generator'], DATA_RAW['generator_copy'] = itertools.tee(DATA_RAW['generator'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Inspection of Datasets\n\n---\n\nWe can now have a look at our two datasets to get a first impression for what kind of data we are dealing with. We start by reporting the number of records and fields/ variables in each of the datasets by using the shape property of the `pd.DataFrame`. We then continue to have an actual look into the data. Similiar to the head command in terminal, we can use the method `head()` onto our DataFrames, which outputs an inline representation of the first five data records of the dataset.",
      "metadata": {
        "tags": [],
        "cell_id": "00015-bc508507-98dd-4277-95d2-820a3765f909",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "**Shape**",
      "metadata": {
        "tags": [],
        "cell_id": "00016-d2dacc61-945c-4362-8d5d-474ff69ead14",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00015-061506db-72d9-42ea-8d71-96b100252925",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "cf497bf3",
        "execution_millis": 0,
        "output_cleared": true,
        "execution_start": 1618981595787,
        "deepnote_cell_type": "code"
      },
      "source": "for dataset in NAMES['datasets']:\n    print(f\"{dataset.capitalize()}: {DATA[dataset].shape}\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "**Diagnosis Dataset**",
      "metadata": {
        "tags": [],
        "cell_id": "00019-32045793-8a8c-4aae-a08f-8a8d0c8482e6",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00020-46b4d62f-7504-43ed-9415-48567333d4fb",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "8e901a74",
        "execution_millis": 18,
        "output_cleared": true,
        "execution_start": 1618981596466,
        "deepnote_cell_type": "code"
      },
      "source": "DATA['diagnosis'].head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00025-19369e86-c8a2-44c4-982f-5ce5a8148e3f",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "881eae63",
        "execution_millis": 77,
        "output_cleared": true,
        "execution_start": 1618981600646,
        "deepnote_cell_type": "code"
      },
      "source": "%%capture\n# add diagnosis to df\n\n# extract cols\nmel = DATA['diagnosis']['melanoma']\nker = DATA['diagnosis']['seborrheic_keratosis']\n\n# mask all cases\nmel_mask = (mel == 1) & (ker == 0)\nker_mask = (mel == 0) & (ker == 1)\nneither_mask = (mel == 0) & (ker == 0)\nboth_mask = (mel == 1) & (ker == 1)\n\nDATA['diagnosis']['diagnosis'] = 0\nfor i in range(DATA['diagnosis'].shape[0]):\n    if mel_mask[i]: DATA['diagnosis']['diagnosis'].loc[i] = 2\n    elif ker_mask[i]: DATA['diagnosis']['diagnosis'].loc[i] = 1\n    else: DATA['diagnosis']['diagnosis'].loc[i] = 0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00022-0c9e3f29-3e98-440d-99b0-d600fc63f164",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "a3ce5907",
        "execution_millis": 147,
        "output_cleared": true,
        "execution_start": 1618981602620,
        "deepnote_cell_type": "code"
      },
      "source": "# distribution of diagnosis\n\ndiagnosis, counts = np.unique(DATA['diagnosis']['diagnosis'], return_counts=True)\n\nfor x in range(len(diagnosis)):\n    print(f\"{diagnosis[x]}: {counts[x]}\")\n\n# plot\nfig,ax = plt.subplots()\nax.bar(diagnosis, counts, color='gray')\n# maybe add text with numeric count\nax.set_title('Distribution of Diagnosis', fontweight='bold'); ax.set_xlabel('Diagnosis', fontstyle='italic'); ax.set_ylabel('Frequency', fontstyle='italic');\nax.set_xticks(diagnosis); ax.set_xticklabels(['Neither', 'Keratosis', 'Melanoma']);",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "As we can see, the majority of observed skin lesions was healthy (78 / 150), one fifth of the skin lesions were diagnosed with melanoma and approximately one fourth was diagnosed with keratosis. \n\nThe two diseases are - as expected - mutually exclusive, meaning that a single skin lesion cannot be diagnosed with multiple diseases.",
      "metadata": {
        "tags": [],
        "cell_id": "00026-e5b563ed-0d60-4d37-ac09-3cc0a1505f27",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "**Features Dataset**\n\nThe features dataset so far only stores the area and perimeter for each given medical image. This dataset will be used to store all of our handcrafted features at a later point in the project.",
      "metadata": {
        "tags": [],
        "cell_id": "00020-08456b29-500b-42b4-bde5-2d0b47ec5417",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00021-e6c8d44e-3740-4368-8a36-7f44f7a1cae6",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "f0f4252b",
        "execution_millis": 17,
        "output_cleared": true,
        "execution_start": 1618981605576,
        "deepnote_cell_type": "code"
      },
      "source": "DATA['features'].head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Inspection of Images\n---\nThe main part of the project is to analyse medical images for a set of features. To do so, we can analyse both the original image and a binary mask provided in the raw data. In this section, we will look at examples of images and their corresponding mask to get a feel for the type of images we are dealing with and assess the quality of the masks.",
      "metadata": {
        "tags": [],
        "cell_id": "00022-74d262e9-0785-418b-a633-7e5351b05a1e",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00023-98a3e227-2bf8-417a-80a6-b3badc4edf43",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "759b8a1",
        "execution_millis": 1256,
        "output_cleared": true,
        "execution_start": 1618981607711,
        "deepnote_cell_type": "code"
      },
      "source": "# load test image using PIL\nfig, ax = plt.subplots(nrows=3, ncols=2, figsize=(15,15))\nfig.suptitle('Examples of Images and Corresponding Masks', fontsize=16, fontweight='bold')\n\nfor i in range(3):\n    ex_img = Image.open('../data/processed/images/' + FILENAME['images'][i] + '.jpg')\n    ex_img_mask = Image.open('../data/processed/masks/' + FILENAME['masks'][i] + '.png')\n\n    ax[i][0].imshow(ex_img)\n    ax[i][0].set_xlabel(f\"{ex_img.format}, {ex_img.size}, {ex_img.mode}\");\n    ax[i][1].imshow(ex_img_mask, cmap='gray')\n    ax[i][1].set_xlabel(f\"{ex_img_mask.format}, {ex_img_mask.size}, {ex_img_mask.mode}\");\n\nax[0][0].set_title('Medical Image');\nax[0][1].set_title('Corresponding Binary Mask');\n\nplt.tight_layout()",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "*TASK 1*\n# Extracting Features \n\n---\n\nWe need to find quantitative measures of how to best classify the skin abnormality *keratosis*. The following handcrafted features are assessed during this project:\n\n> **Compactness**. A quantitative measure of the shape of the lesion. The smaller the value, the more compact the lesion is. A perfect circle has a compactness score of roughly *1.4*.\n\n> **Average Luminance**. A quantitative measure of the averaged brightness of the lesion. The higher the value, the lighter the lesion, and vice versa. Values range from 0 (meaning 100% black) to 255 (meaning 100% white).\n\n> **Luminance Variability**. A quantitative measure to determine the variation of luminance of the lesion. The higher the value, the more variation can be found on the lesion. \n\n> **Average Colour**. A quantitative measure of the averaged colour of the lesion. Values are in RGB format\n\n> **Colour Variability**. A quantitative measure to determine the variation of color of the lesion. The higher the value, the more variation can be found on the lesion. output in the format of (rvariation,gvariation,bvariation),average variation\n\n> **Asymmetry**. A quantitative measure to assess the symmetry of the lesion. Measured through relative number of non-overlapping pixels in different rotations. The higher the value, the less symmmetric the lesion is. A perfect circle, should score a 0 asymmetry score.\n\n> **Border Smoothness**. (WE NEVER GOT THIS)",
      "metadata": {
        "tags": [],
        "cell_id": "00011-2f24366e-2e6e-4fb6-bd80-8420bbf40dde",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "## Functions for Feature Extraction",
      "metadata": {
        "tags": [],
        "cell_id": "00032-577c1d3c-aca3-4b53-b527-01781d1e819a",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00033-50ce0ba4-6e2a-4f83-9209-da3d252a2ab3",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "ef3eec4d",
        "execution_millis": 1,
        "execution_start": 1618981622514,
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "source": "def measure_area_perimeter(mask):\n    # Measure area: the sum of all white pixels in the mask image\n    mask = np.where(np.array(mask)==255, 1, 0)\n    \n    # compute area as number of white pixels \n    area = np.sum(mask)\n\n    # compute perimeter by eroding 1 pixel from mask and then compute the difference between original and eroded mask\n    struct_el = morphology.disk(1)\n    mask_eroded = morphology.binary_erosion(mask, struct_el)\n    image_perimeter = mask - mask_eroded\n    perimeter = np.sum(image_perimeter)\n    \n    return area, perimeter",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00029-d9616ba2-826f-40c2-8d07-5d8ab8fd7c5e",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "b436d025",
        "execution_millis": 1,
        "output_cleared": true,
        "execution_start": 1618981622940,
        "deepnote_cell_type": "code"
      },
      "source": "# compactness\ndef get_compactness(mask):\n    area, perimeter = measure_area_perimeter(mask)\n\n    # return compactness formula\n    return ( ( perimeter ** 2) / (4 * np.pi * area ) )",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00028-ef906370-e43a-41aa-90d7-aa44d8cf16fe",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "ba4da5b5",
        "execution_millis": 1,
        "output_cleared": true,
        "execution_start": 1618981623593,
        "deepnote_cell_type": "code"
      },
      "source": "# average luminance\ndef get_average_luminance(filtered_image): # image needs to be filtered for lesion\n    gray = np.array(filtered_image.convert('L')) # converting to gray scale \n    return round(np.mean(gray[gray > 0]))",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00042-54d6c44d-ae8c-4362-ad59-6e7e30c06b71",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "e39f82f",
        "execution_millis": 4,
        "output_cleared": true,
        "execution_start": 1618981626016,
        "deepnote_cell_type": "code"
      },
      "source": "# luminance variability\ndef get_luminance_variability(filtered_image, measure='variance'):\n    gray = np.array(filtered_image.convert('L')) # converting to gray scale \n    if measure == 'variance': return round(np.var(gray[gray > 0]))\n    elif measure == 'standard_deviation': return round(np.std(gray[gray > 0]))\n    else: print('Cannot compute this `measure`. Try `variance` or `standard_deviation`')",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00056-68ee87e3-71b4-493a-b296-a58f9365fcda",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "a9eb3fdc",
        "execution_millis": 1,
        "output_cleared": true,
        "execution_start": 1618981626024,
        "deepnote_cell_type": "code"
      },
      "source": "# average color\ndef get_average_color(filtered_image): # image needs to be filtered for lesion\n    r, g, b = filtered_image.split() # converting to separate channels  \n    r= np.array(r) \n    g= np.array(g)\n    b= np.array(b) \n    return [round(np.mean(r[r > 0])),round(np.mean(g[g > 0])),round(np.mean(b[b > 0]))]",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00043-e14402fc-ca64-4234-8d4d-9c6a06ddfcb8",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "d728f987",
        "execution_millis": 1,
        "output_cleared": true,
        "execution_start": 1618981627524,
        "deepnote_cell_type": "code"
      },
      "source": "# color variability\ndef get_color_variability(filtered_image, measure='variance'): # image needs to be filteredd for lesion\n    r, g, b = filtered_image.split() # converting to separate channels  \n    r= np.array(r) \n    g= np.array(g)\n    b= np.array(b) \n    if measure == 'variance': rgb=(np.var(r[r > 0]),np.var(g[g > 0]),np.var(b[b > 0]))\n    elif measure == 'standard_deviation': rgb=(np.std(r[r > 0]),np.std(g[g > 0]),np.std(b[b > 0]))\n    else: return \n    return np.mean(rgb)",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00038-30509f12-2361-4e03-804c-cdc34f4aea6f",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "1dbe74ba",
        "execution_millis": 0,
        "output_cleared": true,
        "execution_start": 1618981627918,
        "deepnote_cell_type": "code"
      },
      "source": "def get_asymmetry(mask):\n    return round(np.mean([asymmetry(mask), asymmetry(mask.rotate(90, expand=True))]),2)",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00030-7ee66c85-2792-4ad4-935f-dfc3e8257a30",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "ce804e57",
        "execution_millis": 1,
        "output_cleared": true,
        "execution_start": 1618981628684,
        "deepnote_cell_type": "code"
      },
      "source": "# helper for asymmetry\ndef asymmetry(mask):\n    # calculate basic properties of image\n    width, length = mask.size  # requires even number of pixels in both dimensions\n    size = width * length\n\n    if width%2!=0:\n        print(\"Uneven Number of Pixels. Can't calculate asymmetry.\")\n\n    # cut in half and fold\n    left = mask.crop((0,0,(width/2), length)) \n    right = mask.crop((width/2,0,width,length))\n    right = right.transpose(Image.FLIP_LEFT_RIGHT)\n\n    # get binary array of unequal positions (the closer the sum to 0, the better the symmetry)\n    diff = np.where(np.array(left) != np.array(right), 1, 0)\n\n    return np.sum(diff) / size # percentage of asymmetric pixels",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Evaluating Feature Extraction\n---\n\nHere we test the quality of the functions on sample lesions and dummy images.",
      "metadata": {
        "tags": [],
        "cell_id": "00055-1858f146-2046-4965-b2cc-f966cab2caf6",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00042-1eae9862-d608-44ec-b89e-54ecf331b7b9",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "31dd0608",
        "execution_millis": 1221,
        "execution_start": 1618981633489,
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "source": "dummies = ['dummy' + str(i+1) for i in range(2)]\n\n# show dummy image and mask\nfig, ax = plt.subplots(nrows=len(dummies), ncols=2, figsize=(16, 6 * len(dummies)))\nfig.suptitle(\"Feature Extration on Dummies\", fontsize=16, fontweight='bold')\n\nfor i, img_name in enumerate(dummies):\n    # compute features\n    img = Image.open('../data/external/dummy_images/' + img_name + '.jpg')\n    img_mask = Image.open('../data/external/dummy_images/' + img_name + '_mask.png').convert('L')\n\n    area, perimeter = measure_area_perimeter(img_mask)\n    compactness = get_compactness(img_mask)\n    asymmetry_score = get_asymmetry(img_mask)\n    average_luminance = get_average_luminance(img)\n    luminance_variance = get_luminance_variability(img)\n    average_colors = get_average_color(img)\n    color_variance = get_color_variability(img)\n\n    # print out computed features\n    print('#'*15 + f\"   Dummy {i+1}   \" + '#'*15)\n    print(f\"Area / Perimeter      : {area}px / {perimeter}px\")\n    print(f\"Compactness           : {compactness}\")\n    print(f\"Asymmetry             : {asymmetry_score}px\")\n    print(f\"Average Luminance     : {average_luminance}\")\n    print(f\"Luminance Variance    : {luminance_variance}\")\n    print(f\"Average Colors (RGB)  : {average_colors}\")\n    print(f\"Color Variability     : {color_variance}\\n\")\n\n    ax[i][0].imshow(img);\n    ax[i][1].imshow(img_mask, cmap='gray');\n    ax[i][0].set_title(f'Dummy {i+1}', fontsize=10, fontstyle='italic')\n    ax[i][1].set_title(f'Dummy {i+1} Mask', fontsize=10, fontstyle='italic')\n    ax[i][0].set_xlabel(f'{img.format}, {img.size}, {img.mode}', fontstyle='italic');\n    ax[i][1].set_xlabel(f'{img_mask.format}, {img_mask.size}, {img_mask.mode}', fontstyle='italic');\n    plt.tight_layout()",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Compute Features\n---\n\nWe have developed our functions to extract abstracted features from the medical images of the lesions and have proven them to work on dummy images, where we were able to validate assumed scores in each feature. We can therefore now compute the features for each image and append it to our main dataframe to store the data, that is later used to train our model. \n\nWe first read in the measured features into a dictionary `feature_dict`, which we later concatenate horizontally with `features.csv` dataset. ",
      "metadata": {
        "tags": [],
        "cell_id": "00053-e69f91a1-e003-4880-bc8e-9ccdc2a51e04",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00059-ff38722b-dbe7-43fd-85ea-453cf62b1180",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "2d7e6f16",
        "execution_millis": 4,
        "output_cleared": true,
        "execution_start": 1618981647761,
        "deepnote_cell_type": "code"
      },
      "source": "features = ['compactness', 'average_luminance', 'luminance_variability', 'average_red', 'average_green', 'average_blue', 'color_variability', 'asymmetry']\n\nfeature_functions = {\n    'compactness': get_compactness,\n    'average_luminance': get_average_luminance,\n    'luminance_variability': get_luminance_variability,\n    'average_red': get_average_color,\n    'average_green': get_average_color,\n    'average_blue': get_average_color,\n    'color_variability': get_color_variability,\n    'asymmetry': get_asymmetry\n}",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00054-4dafbcbe-8adc-43ff-942e-82502bfbf355",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "ff8e3abd",
        "execution_millis": 58446,
        "output_cleared": true,
        "execution_start": 1618981651112,
        "deepnote_cell_type": "code"
      },
      "source": "# compute all features (execution time: 60s)\nif COMPUTE_FEATURES == True:\n    feature_dict = {feature: [] for feature in features}\n\n    for i in range(len(FILENAME['images'])):\n        name = FILENAME['images'][i]\n        img_name = FILENAME['images'][i] + '.jpg'\n        mask_name = FILENAME['masks'][i] + '.png'\n\n        # open temporarily\n        filtered_img = Image.open(PATH['data']['processed'] + PATH['filtered_images'] + img_name)\n        mask = Image.open(PATH['data']['processed'] + PATH['masks'] + mask_name)\n\n        # measure features and append to feature_dict\n        for feature in features:\n            if feature in ['compactness', 'asymmetry']:\n                feature_dict[feature].append(feature_functions[feature](mask))\n            elif feature in ['average_luminance', 'luminance_variability', 'color_variability']:\n                feature_dict[feature].append(feature_functions[feature](filtered_img))\n            elif feature == 'average_red':\n                feature_dict[feature].append(feature_functions[feature](filtered_img)[0])\n            elif feature == 'average_green':\n                feature_dict[feature].append(feature_functions[feature](filtered_img)[1])\n            elif feature == 'average_blue':\n                feature_dict[feature].append(feature_functions[feature](filtered_img)[2])",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Build Main DataFrame\n---\n*add small description*",
      "metadata": {
        "tags": [],
        "cell_id": "00054-91ebedfb-45da-4415-a8a8-795538fbefa6",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00054-816c0c91-2ea8-4e84-a307-0203ed5b4ce3",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "15fc2a93",
        "execution_millis": 1,
        "output_cleared": true,
        "execution_start": 1618981712864,
        "deepnote_cell_type": "code"
      },
      "source": "# append extracted features and diagnosis to features.csv\nif DATA['features'].shape[1] <= 3:\n    DATA['features'] = pd.concat([DATA['features'], pd.DataFrame(feature_dict)], axis=1) # concatenating handcrafted features\n    DATA['features'] = pd.concat([DATA['features'], DATA['diagnosis'][['melanoma', 'seborrheic_keratosis', 'diagnosis']]], axis=1) # concatenating diagnosis",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00060-344598b0-1f7c-4353-997d-46f4c12178aa",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "4c274380",
        "execution_millis": 74,
        "execution_start": 1618981713684,
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "source": "DATA['features']",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Normalise Features\n---\nWhen training our ML model, it is important that each feature equally contributes into the process of classification. Since features are on very different scales (*ie. 0-255 for Luminance/ Color or #Pixels for Asymmetry*), we normalise them using sklearn's `StandardScaler()` to normalise all numeric columns in the DataFrame `features.csv` and save the normalised DataFrame into `DATA['scaled_features']`.",
      "metadata": {
        "tags": [],
        "cell_id": "00058-83590354-e61c-4591-b0ce-b3b5d2da2efd",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00059-dd57850c-079a-4934-ac11-4d1d4a8e6ea6",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "c398453f",
        "execution_millis": 73,
        "execution_start": 1618981719652,
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "source": "scaler = StandardScaler().fit(DATA['features'].loc[:, DATA['features'].columns != 'id'])\nDATA['scaled_features'] = scaler.transform(DATA['features'].loc[:, DATA['features'].columns != 'id'])\nDATA['scaled_features'] = pd.DataFrame(DATA[\"scaled_features\"], columns=list(DATA['features'])[1:]) # convert back to pd.DataFrame\nDATA['scaled_features']",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Saving DataFrame\n---\nSave both `feature` dataframes into `csv` format into `../data/processed`.",
      "metadata": {
        "tags": [],
        "cell_id": "00058-53e896d2-345c-473c-a33a-940751b7b9e5",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00064-6c9b457d-f0e0-41d6-aede-4c392f51d8fb",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "2488186a",
        "execution_millis": 14,
        "execution_start": 1618981732211,
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "source": "# save features and scaled features\nDATA['features'].to_csv(PATH['data']['processed'] + 'features.csv')\nDATA['scaled_features'].to_csv(PATH['data']['processed'] + 'scaled_features.csv')",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "*TASK 1.5*\n# Evaluate Features for Classification\n---\n\nIn this section, we are plotting the handcrafted features in order to see, whether any correaltions appear. Since we are trying to classify a binary label, namely whether a lesion is diseased with *melanoma* or not, we plot the distribution of each feature in the two classes *melanoma* and *no melanoma* to get a visual intuition of how good the specific feature might be able to distinguish between our target label. \nThe more distinct the two distributions are, the better the feature will be at prediciting the tartget value.",
      "metadata": {
        "tags": [],
        "cell_id": "00053-69c9fc88-3be3-4112-8c01-f0dbb64ad41a",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Visual Exploration",
      "metadata": {
        "tags": [],
        "cell_id": "00062-7e6b9ce6-6d64-4bad-9689-2e22cb93bfbf",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00056-59589987-30e3-42b4-b59a-04f452c8a344",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "bb95ba7a",
        "execution_millis": 1807,
        "output_cleared": true,
        "execution_start": 1618981735158,
        "deepnote_cell_type": "code"
      },
      "source": "fig, ax = plt.subplots(nrows=len(features) + 2, figsize=(8, 6 * len(features) + 2))\n\nfor i, feature in enumerate(features + ['area', 'perimeter']):\n    sns.violinplot(x='melanoma', \n                   y=feature, \n                   data=DATA['features'],  \n                   ax=ax[i]).set_title(f\"Distribution of {feature.title().replace('_', ' ')} in Different Classes\");",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00067-14466d2a-d096-4e81-a4d6-25b9d15b5e50",
        "deepnote_to_be_reexecuted": true,
        "source_hash": "6af9874d",
        "execution_millis": 47781,
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "source": "sns.pairplot(DATA['features'], hue='melanoma', height=2, diag_kind=\"hist\"); ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "*TASK 2*\n# Predict Diagnosis\n\n---\n\nWe have 'handcrafted' a number of features and now aim to find the model that is the most robust to predict our target variable - whether the lesion is our is not diseased with melanoma. In the process of doing so, a number of questions arise that we will step-by-step tackle in order to build the 'best' model. \n\n\n\n> **Selecting Features**: Which combination of features is likely do perform best?\n\n> **Normalisation**: How do we make sure that each feature contributes equally to the model's prediction?\n\n> **Which model does best in predicting our variable?**\n\n> **Which hyperparameters should we use to create the best model?**\n\n> **How do we evaluate the model? Which metrics do we use to assess whether or not our model does good?**",
      "metadata": {
        "tags": [],
        "cell_id": "00012-7fd9c2d5-6342-493a-af45-dc85efb4d664",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "1. decide on _features_ and _target_ # features at random,but should decide for k best\n#use feature selector or do randomly\n2. normalise features \n3. _split into test, train and do cross validation (split into 5 sets)_\n4. _evaluate_\nhttps://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html",
      "metadata": {
        "tags": [],
        "cell_id": "00069-9a395148-3004-4119-9aef-553c69391bdb",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "source": "!pip install imblearn",
      "metadata": {
        "tags": [],
        "cell_id": "00065-48965e58-ffcc-48e7-804e-1af961ab4864",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "73efdfa6",
        "execution_start": 1618982810605,
        "execution_millis": 3344,
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "from imblearn.under_sampling import RandomUnderSampler # balancing data\nfrom imblearn.over_sampling import RandomOverSampler # balancing data\n\nfrom sklearn.feature_selection import SequentialFeatureSelector\nfrom sklearn.feature_selection import SelectKBest\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score \nfrom sklearn.model_selection import StratifiedShuffleSplit #for cross-validation\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix",
      "metadata": {
        "tags": [],
        "cell_id": "00065-84413489-f7a3-4a3d-9b8d-7792446ea447",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "acc63d0b",
        "execution_millis": 4,
        "execution_start": 1618987523896,
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "### Define Initial Features and Target Variable\n---\nFirst, we need to define from which set of features our model should choose the ones that are likely to perform best on a specified target variable. \nIn our case, each measured feature is potentially interesting for the model, and we want to predict the binary label, whether or not the classifier is diseased with *melanoma* or not.\n\nBy, convention, we call the feature matrix `X` and the target column `y`.",
      "metadata": {
        "tags": [],
        "cell_id": "00067-01107798-cc24-4632-a815-3423c563b1f0",
        "deepnote_cell_type": "markdown"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "X = DATA['features'][features]\ny = DATA['features'][['melanoma']]",
      "metadata": {
        "tags": [],
        "cell_id": "00068-a4ea27ed-35f8-42a1-8ce0-beed82d2528b",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "1274c265",
        "execution_millis": 4,
        "execution_start": 1618985364157,
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "### Normalisation\n---\nOur measured handcrafted features all perform on different scales, ie. the *average luminance* can only obtain values between 0 and 255, whereas the *asymmetry* is the number of non-matching pixels in two-folds.\n\nSince we want each of our features to equally contribute to the prediction of the model, we need to normalise them. We do this through the formula of the standard score $\\frac{x-\\mu}{\\sigma}$ ([Wikipedia](https://en.wikipedia.org/wiki/Standard_score)).\n\nWe can easily use `sklearn`'s `StandardScaler`, which does the job for us.",
      "metadata": {
        "tags": [],
        "cell_id": "00077-b097ada6-58a9-40c9-90ca-d8dcbad7434a",
        "deepnote_cell_type": "markdown"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# distribution of features before normalisation\nplt.figure(figsize=(15,5));\nsns.boxplot(data=X, width=0.5);",
      "metadata": {
        "tags": [],
        "cell_id": "00070-1542bccc-2f57-49fb-b39d-f1844e915367",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "6444a681",
        "execution_millis": 382,
        "execution_start": 1618985497685,
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "scaler = StandardScaler().fit(X) # (x - mu) / std \nX = pd.DataFrame(scaler.transform(X), columns=features)",
      "metadata": {
        "tags": [],
        "cell_id": "00078-c08a7e1c-a0ee-4d29-9ea6-3ee1b70ea292",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "41bfd634",
        "execution_millis": 4,
        "execution_start": 1618985539188,
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# distribution of features after normalisation\nplt.figure(figsize=(15,5));\nsns.boxplot(data=X, width=0.5);",
      "metadata": {
        "tags": [],
        "cell_id": "00072-d68daeff-2cd4-4673-8d0f-997340d766de",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "5fba9c86",
        "execution_millis": 422,
        "execution_start": 1618985541102,
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "### Train-Test Split\n---\nBefore, we doing anything further with our data, we split our data into a `development` set and a `test` set. The test set will be used to assess the final performance of the model by mimicing a true *out-of-sample* set of datapoints. For this project, we chose a standard split size of `0.3`, meaning that 30% of the original data will be used for testing, and 70% for the development of the model.\n\nWe split the data at this early stage to not bias our model towards the test dataset, ie. performing the feature selection process on all of the data, might bias it towards the test data, and thus result in a sligthly overfitted model, that might perform good on the test, but not in real-life. \n\n*Note: Whenever there is randomness involved, we set `random_state=1` to produce reproducable results.",
      "metadata": {
        "tags": [],
        "cell_id": "00065-dc7dbdd2-dbd1-4a7d-a7e8-9f93e056de62",
        "deepnote_cell_type": "markdown"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# split into train-test data (hold-out data to mimic true 'out-of-sample' data)\nX_dev, X_test, y_dev, y_test = train_test_split(X, y, test_size=0.3, random_state=1)",
      "metadata": {
        "tags": [],
        "cell_id": "00066-81016beb-b954-4925-a39e-26939fbbc303",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "d91cf24",
        "execution_millis": 5,
        "execution_start": 1618985554373,
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "print(X_dev.shape, y_dev.shape)\nprint(X_test.shape, y_test.shape)",
      "metadata": {
        "tags": [],
        "cell_id": "00071-9bf60f5a-cb0b-449e-8c72-21ce9ac26108",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "4b5f9155",
        "execution_millis": 61,
        "execution_start": 1618985554790,
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "### Balancing Data\n---\nAs we have seen in the initial exploration of the data, the labels are heavily imbalanced. Out of the 150 observed images, only 30 are diseased with *melanoma*, which results in precisely 20% of the data. If we would naively train our model disregarding the imbalance and only evaluate the model's performance by its *accuracy score*, we are likely to see a model performing equally or very similar to the so-called *null-hypotheses* (always predicting the dominating label), which would lead to a 80% accuracy in our case. \n\nTo prevent this we balance our data, such that at the end of the process the data we train our model has equal amounts of *melanoma* and *not melanoma*.\nThere are two ways of doing this:\n\n> **Random Undersampling**: Cutting down the frequency of the more dominant label.\n\n> **Random Oversampling**: Duplicating the less dominant feature.\n\nAlthough, both option are obviously not ideal (since we loose information in the first, and create duplicate information in the second), and we would always prefer a orginally balanced dataset, we need to make a choice. For this project, we *upsampled* our data, since undersampling, would have limited our whole analyis to only 60 images, which is likely to not create an accurate model.\n\nWe oversample using `imblearn`'s class `RandomOverSampler`.",
      "metadata": {
        "tags": [],
        "cell_id": "00072-494c736c-5e09-4192-b2df-51ccb2c77ecd",
        "deepnote_cell_type": "markdown"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# plot before oversampling\nfig, axes = plt.subplots(ncols=3, figsize=(12, 3))\nfor ax, dataset in zip(axes, [y, y_dev, y_test]):\n    unique, counts = np.unique(dataset, return_counts=True)\n    ax.bar(unique, counts); ax.set_xticks([1,0])\naxes[0].set_title('Target Imbalance on `y`');\naxes[1].set_title('Target Imbalance on `y_dev`');\naxes[2].set_title('Target Imbalance on `y_test`');",
      "metadata": {
        "tags": [],
        "cell_id": "00073-e32b4d9f-7582-472e-a642-bc0034c1930f",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "e3655ee9",
        "execution_millis": 991,
        "execution_start": 1618985556804,
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# balance data (using over-sampling)\noversampler = RandomOverSampler(random_state=1)\nX_dev_sampled, y_dev_sampled = oversampler.fit_resample(X_dev, y_dev)",
      "metadata": {
        "tags": [],
        "cell_id": "00074-f86d0a82-4a9f-43f5-8b77-1b6558caf968",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "6cd0718d",
        "execution_millis": 0,
        "execution_start": 1618985974114,
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# plot before oversampling\nfig, axes = plt.subplots(ncols=3, figsize=(12, 3))\nfor ax, dataset in zip(axes, [y, y_dev_under, y_test]):\n    unique, counts = np.unique(dataset, return_counts=True)\n    ax.bar(unique, counts); ax.set_xticks([1,0])\naxes[0].set_title('Target Imbalance on `y`');\naxes[1].set_title('Target Balance on `y_dev_under`');\naxes[2].set_title('Target Imbalance on `y_test`');",
      "metadata": {
        "tags": [],
        "cell_id": "00075-04726444-b966-413f-9e89-e4c3432b5896",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "dd7a8771",
        "execution_millis": 452,
        "execution_start": 1618985975235,
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "print(f'#Images before Upsampling:   {X_dev.shape[0]}')\nprint(f'#Images after Upsampling:    {X_dev_sampled.shape[0]}')\nprint(f'Upsampled Images:            {X_dev_sampled.shape[0]-X_dev.shape[0]}')",
      "metadata": {
        "tags": [],
        "cell_id": "00076-73db88b0-4ba0-45e3-8b73-02925851c6a7",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "9ce40e35",
        "execution_millis": 18,
        "execution_start": 1618985985091,
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Feature Selection\n---\n\n*Feature Selection* is the process of identifying the most related features from the data and removing the irrelvant or less important features which do not contribute much to our target variable.\n\nThere are a number of advantages that come along with feature selection, which make it an important step of every machine learning project.\n\n1. **Reduces Overfitting**: Less redundant data means less opportunity to make decisions based on noise.\n\n2. **Improves Accuracy**: Less misleading data means modeling accuracy improves.\n\n3. **Reduces Training Time**: fewer data points reduce algorithm complexity and algorithms train faster.\n\n",
      "metadata": {
        "tags": [],
        "cell_id": "00065-f41ea02b-0bc5-4b45-9c2f-7b25dcb838d9",
        "deepnote_to_be_reexecuted": true,
        "source_hash": "8c0913b",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "### K-Best\n---\n\nStatistical tests can be used to select those features that have the strongest relationship with the output variable.\nThe scikit-learn library provides the `SelectKBest` class that can be used with a suite of different statistical tests to select a specific number of features.\n\nIn order to evaluate our features with respect to randomly generated data, we add some uniformly random columns to our development data, in order to select the features that are likely to perform best. \n\nIt is however important to notice, that K-Best Selection is a way of selecting univariate features, meaning that it only checks the likely performance for each single feature. This method might ie. give us 5 individually good performing features, which are highly correlated. In that case, the 4 additionally selected features, don't add any value to our model and thus increase running time and potentially accuracy. ",
      "metadata": {
        "tags": [],
        "cell_id": "00075-9ce8c66f-4892-46ad-b6f7-1983a4a8e83c",
        "deepnote_cell_type": "markdown"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "FEATURES_SELECTED = {}\nFEATURES_SELECTED['kbest'] = {}\nFEATURES_SELECTED['sequential'] = {}",
      "metadata": {
        "tags": [],
        "cell_id": "00083-2e73bd54-f217-4842-ac77-df5e87e10bc4",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "d269b20f",
        "execution_start": 1618988327745,
        "execution_millis": 0,
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00068-853c950c-08b2-44b9-9c38-171085a681ec",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "45dfa306",
        "execution_millis": 1618,
        "execution_start": 1618987208058,
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "source": "# generate some noise\nnoise_cols = 4\nnoise = np.random.RandomState(1).uniform(0,0.1, size=(noise_cols, X_dev_sampled.shape[0])).transpose()\n\n# feature matrix and target variable\nX_select = np.hstack((X_dev_sampled, noise))\ny_select = y_dev_sampled\n\n# plot scores for features and noise\nfig, ax = plt.subplots(ncols=5, figsize=(4*5, 3))\nfig.suptitle('Univariate Feature Scores for Features and Noise for different K-Best', fontsize=14)\n\n# select k-best\nfor k in range(1,5+1):\n    kbest = SelectKBest(mutual_info_classif, k=k)\n    kbest.fit(X_select, np.ravel(y_select)) # ravel cause of sklearn warning \n    scores = kbest.scores_ # univariate features scores for each feature and noise\n\n    print('-'*10 + f' k = {k} ' + '-'*10)\n    scores_dict = {scores[i]: i for i in range(len(scores))}\n    for val in sorted(scores_dict, reverse=True)[:k]:\n        print(features[scores_dict[val]])\n\n    ax[k-1].bar(np.arange(0,len(scores)), scores)\n    ax[k-1].tick_params(labelrotation=90) # readable labels\n    ax[k-1].set_xticks(np.arange(0, len(features) + noise_cols));\n    ax[k-1].set_xticklabels([f.replace('_', ' ').title() for f in features] + ['Noise' for _ in range(noise_cols)]);",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Feature Importance using Forward Sequential Feature Selector\n---\n*description*",
      "metadata": {
        "tags": [],
        "cell_id": "00074-91001a41-b5d6-4bb4-aa89-2a7897ec22ad",
        "deepnote_cell_type": "markdown"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "features_noise = features + ['Noise' for _ in range(noise_cols)]",
      "metadata": {
        "tags": [],
        "cell_id": "00086-1a9ce2a2-424a-46c9-aae4-6ea96dd5e973",
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "for k_select in range(1,6):\n    forward_selection = SequentialFeatureSelector(KNeighborsClassifier(n_neighbors=5), n_features_to_select=k_select)\n    forward_selection.fit(X_select, np.ravel(y_select))\n    chosen=forward_selection.get_support()\n    print('-'*10 + f' k = {k_select} ' + '-'*10)\n    print(np.array(features_noise)[chosen])",
      "metadata": {
        "tags": [],
        "cell_id": "00086-a76a17be-d52b-46f6-b51f-70cc133a89d3",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "3ff26cb9",
        "execution_millis": 4635,
        "execution_start": 1618987910230,
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "############################ trash?\n# Feature Importance\nfrom sklearn import datasets\nfrom sklearn import metrics\nfrom sklearn.tree import ExtraTreeClassifier\n# load the iris datasets\n# fit an Extra Tree model to the data\nmodel = ExtraTreeClassifier()\nmodel.fit(X, y)\nmodel.decision_path(X)\n# display the relative importance of each attribute\nscores=model.feature_importances_\nfig, ax = plt.subplots()\nax.bar(np.arange(0,len(scores)), scores, width=.2,label=r'Feature score')\nplt.xticks(rotation = 90) # Rotates X-Axis Ticks by 45-degrees\nax.set_xticks(np.arange(0, len(features) + noise_cols));\nax.set_xticklabels(features + ['Noise' for _ in range(noise_cols)]);\n#############################",
      "metadata": {
        "tags": [],
        "cell_id": "00075-06ac76e6-e455-478f-9493-e73ae3aa15b2",
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "### Building KNN-Model\n\nFor the scope of this project, we first limit ourselves to a basic classifier called `KNN` (*K-Nearest-Neighbors*) algorithm, classifies out-of-sample data by evaluating the label frequency of k-surrounding (mostly measured through *euclidean distance*) neighbors in a n-dimensional space (where n is the number of features).\n\nTo build the model, we choose the features selected by *K-Best Features* and and train our model using cross-validation of 10 partitions. In that way we get a more robust estimate for the performance of the metric. \n\nWe will evaluate the final performance on the test-data that was separated in the beginning of this section, to give an estimate of the performance of our model on real-life data.",
      "metadata": {
        "tags": [],
        "cell_id": "00072-900e0e6a-2251-4e02-9d77-771007f4cb09",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "source": "selected_features = ['average_red', 'compactness']",
      "metadata": {
        "tags": [],
        "cell_id": "00090-6f3f3921-47fb-4a2c-99c8-cc6b3980c0a7",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "5b7dd010",
        "execution_millis": 5,
        "execution_start": 1618989546662,
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "X_train, y_train = X_dev_sampled[selected_features], np.ravel(y_dev_sampled) # redfine for simplicity\n\nfor k in range(1,11):\n    knn = KNeighborsClassifier(n_neighbors=k)\n    scores = cross_val_score(knn, X_train, y_train, cv=5, scoring='roc_auc') # roc_auc looks at sensitivity and specificity\n    print(np.mean(scores), np.std(scores))",
      "metadata": {
        "tags": [],
        "cell_id": "00089-7de6a23a-95b3-4e03-93fa-eb0e7de32128",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "9c1c018b",
        "execution_millis": 980,
        "execution_start": 1618989547320,
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "knn = KNeighborsClassifier(n_neighbors=2)\nknn.fit(X_train, y_train)\nprint(X_train.shape, y_train.shape)\nprint(X_test.shape)\n\ntest_predicted = knn.predict(X_test[selected_features])\nconfusion = confusion_matrix(y_test, test_predicted)\ntp = confusion[1][1] \ntn = confusion[0][0] \nfp = confusion[0][1] \nfn = confusion[1][0] \n\n#print(test_predicted)\n#print(y_test)\n\n# accuracy\naccuracy = (tp + tn) / len(test_predicted) # equivalent to sklearn.metrics.accuracy_score() \n\n# error \nerror = (fp + fn) / len(test_predicted) # \n\n# sensitivity (also: true-positive-rate or recall)\n# how many of the positives did our model detect correctly?\nsensitivity = tp / (tp + fn)\n\n# specificity (also: )\n# how many of the negatives did our model detect correctly?\nspecificity = tn / (tn + fp)\n\nprint(accuracy)\nprint(error)\nprint(sensitivity)\nprint(specificity)",
      "metadata": {
        "tags": [],
        "cell_id": "00091-47587e16-dcca-4a6f-952c-7ead42e7a28c",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "65da7",
        "execution_millis": 110,
        "execution_start": 1618989554315,
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00072-fb0a5589-b261-437a-8a19-b149dde9fcdd",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "8bf3a253",
        "execution_millis": 41,
        "execution_start": 1618989629876,
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "source": "def build_knn(features, target=['melanoma'], k=5, scale=True):\n    # define predictive and target labels\n    X = np.array(DATA['features'][features]) # feature matrix\n    y = np.ravel(DATA['features'][target]) # target column\n\n    print(X.shape, y.shape)\n\n    # balance data (using over-sampling)\n    oversampler = RandomOverSampler(random_state=1)\n    X, y = oversampler.fit_resample(X,y)\n\n    print(X.shape, y.shape)\n\n    # split into train-test data (hold-out data to mimic true 'out-of-sample' data)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n\n    if scale:\n    # normalise features\n        scaler = StandardScaler().fit(X) # (x - mu) / std \n        X = scaler.transform(X)\n    \n    knn = KNeighborsClassifier(n_neighbors=k)\n    scores = cross_val_score(knn, X_train,y_train, cv=10, scoring='roc_auc')\n    print(scores)\n\n    #param_dict = {'n_neighbors': range(1,20)}\n    #grid = GridSearchCV(knn, param_dict, cv=10, scoring='roc_auc')\n    #grid.fit(X_train, y_train)\n    #print(grid.best_estimator_)\n    #print(grid.best_params_)\n    #print(grid.best_score_)\n\n    # build best_k model and fit entire training data\n    #best_k = grid.best_params_['n_neighbors']\n    best_knn = KNeighborsClassifier(n_neighbors=k)\n    best_knn.fit(X_train, y_train)\n\n    # predict test data\n    predicted = best_knn.predict(X_test)\n    #print(predicted)\n    #print(y_test)\n    confusion = confusion_matrix(y_test, predicted)\n    tp = confusion[1][1] \n    tn = confusion[0][0] \n    fp = confusion[0][1] \n    fn = confusion[1][0] \n\n    # accuracy\n    accuracy = (tp + tn) / len(predicted) # equivalent to sklearn.metrics.accuracy_score() \n\n    # error \n    error = (fp + fn) / len(predicted) # \n\n    # sensitivity (also: true-positive-rate or recall)\n    # how many of the positives did our model detect correctly?\n    sensitivity = tp / (tp + fn)\n\n    # specificity (also: )\n    # how many of the negatives did our model detect correctly?\n    specificity = tn / (tn + fp)\n\n    print(accuracy)\n    print(error)\n    print(sensitivity)\n    print(specificity)\n\n    #scores = cross_val_score(knn, X_train, y_train, cv=10, scoring='accuracy')\n    #specificity = cross_val_score(_model, X_train, y_train, cv=10, scoring='')\n    #sensitivity = cross_val_score(_model, X_train, y_train, cv=10, scoring='')",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00074-5cb28359-6777-4d3e-b043-11eba23139ec",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "37de42f8",
        "execution_millis": 136,
        "execution_start": 1618989630249,
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "source": "build_knn(features=['average_luminance', 'luminance_variability', 'asymmetry', 'average_green', 'average_blue'], target=['melanoma'], k=1, scale=True)\n# okay, our model is robust and good\n\n#knn1.predict([239849]) => 1, 0\n\n# so we can train all our train and give final score on test\n#knn.train(x_train, y_train)\n#knn.predict(x_test, y_test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "*TASK 3*\n# Open Question: ...\n\n---\nMany things that we could investigate:\n- test different classifiers\n- try to predict keratosis / or 3-class classification\n- play around with sensitivity and specificity\n- ",
      "metadata": {
        "tags": [],
        "cell_id": "00013-facea51d-4f84-4003-a25d-67505e0f0718",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "",
      "metadata": {
        "tags": [],
        "cell_id": "00025-86c4f020-f4de-43e0-88d0-d1f5b1b0874e",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "",
      "metadata": {
        "tags": [],
        "cell_id": "00026-bed7848d-d805-47f0-8bf7-523a4cb790f6",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "",
      "metadata": {
        "tags": [],
        "cell_id": "00027-65d1e65c-b9c5-4a9a-83da-9c4055f03f8c",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "",
      "metadata": {
        "tags": [],
        "cell_id": "00028-64a0d452-fc0a-40c4-a3b2-4d35e7515feb",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "",
      "metadata": {
        "tags": [],
        "cell_id": "00029-c9887334-2738-44bb-90a3-2b94c9240649",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "",
      "metadata": {
        "tags": [],
        "cell_id": "00030-3232008b-99b3-4ce6-8bb7-627d3c530a81",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "",
      "metadata": {
        "tags": [],
        "cell_id": "00031-7b2a6ffd-3d62-4996-a0b1-43c7ff7c2ae3",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "",
      "metadata": {
        "tags": [],
        "cell_id": "00032-1917b0a3-467d-4259-82c3-12a15ef58ae8",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "",
      "metadata": {
        "tags": [],
        "cell_id": "00033-31391a6a-012a-420c-bc8d-d5892f0044e7",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "",
      "metadata": {
        "tags": [],
        "cell_id": "00034-e91b070c-81b2-401d-9d9b-b166fd16647c",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=e5a81e04-857d-4bae-844e-8fb924df483a' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "tags": [],
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "orig_nbformat": 2,
    "deepnote_notebook_id": "7810e8fe-98e9-4f91-8d21-605a64b16e29",
    "deepnote": {},
    "deepnote_execution_queue": []
  }
}